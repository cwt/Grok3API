# ğŸ§  Grok3API OpenAI-Compatible Server

> ğŸ¤– **Early stage of server development**

The first draft of a server compatible with the OpenAI API (`/v1/chat/completions`) has been implemented.

> âš ï¸ **It's in the early stage and not fully debugged. Use it at your own risk!**

---

### âš™ï¸ Running the Server

> Make sure you have `fastapi`, `uvicorn`, `pydantic`, and `grok3api` installed.

```bash
# From the project root:
python -m grok3api.server
```

```bash
# Or you can configure the host and port
python -m grok3api.server --host 127.0.0.1 --port 9000
```

ğŸ‰ The server will start at `http://127.0.0.1:9000`.

> Uses `uvicorn` under the hood. Make sure the project structure allows for module-based execution (via `-m`).

---

## ğŸ” Endpoint: `/v1/chat/completions`

Compatible with the OpenAI request format:

### ğŸ“¥ Example request:

```python
from openai import OpenAI
from openai import OpenAIError

# Creating a client with the specified URL and key
client = OpenAI(
    base_url="http://localhost:9000/v1",
    api_key="dummy"
)

try:
    # Sending a request to the server
    response = client.chat.completions.create(
        model="grok-3",
        messages=[{"role": "user", "content": "What is Python?"}]
    )

    # Displaying the response from the server
    print("Server Response:")
    print(f"Model: {response.model}")
    print(f"Message: {response.choices[0].message.content}")
    print(f"Finish Reason: {response.choices[0].finish_reason}")
    print(f"Usage: {response.usage}")

except OpenAIError as e:
    print(f"Error: {e}")
except Exception as e:
    print(f"Unexpected error: {e}")
```

> `stream` is not supported, a `400 Bad Request` will be returned.

---

### ğŸ“¤ Output:

```
Server Response:
Model: grok-3
Message: Python is a high-level, interpreted programming language with a simple and readable syntax. It supports multiple programming paradigms (object-oriented, functional, procedural) and is widely used for web development, data analysis, task automation, machine learning, and more. Python is popular for its versatility, extensive standard library, and large developer community.
Finish Reason: stop
Usage: CompletionUsage(completion_tokens=46, prompt_tokens=3, total_tokens=49, completion_tokens_details=None, prompt_tokens_details=None)
```

---


## ğŸ§µ Endpoint: `/v1/string` (GET)

A simple text GET request. Accepts a string as a query parameter and returns a response from Grok without JSON wrapping.

### ğŸ“¥ Example Request:

Open in a browser or use `curl`:

```bash
curl http://localhost:9000/v1/string?q=Hello
```

### ğŸ“¤ Example Response:

```
Hello! How can I help?
```

### ğŸ Example in Python:

```python
import requests

response = requests.get("http://localhost:9000/v1/string", params={"q": "Tell a joke"})
print(response.text)  # Just text, no JSON
```

---

## ğŸ§µ Endpoint: `/v1/string` (POST)

A simple text POST request. Accepts a string in the request body and returns a response from Grok without JSON wrapping.

### ğŸ“¥ Example Request:

Use `curl` or another tool:

```bash
curl -X POST http://localhost:9000/v1/string -d "Hello"
```

### ğŸ“¤ Example Response:

```
Hello! How can I help?
```

### ğŸ Example in Python:

```python
import requests

response = requests.post("http://localhost:9000/v1/string", data="Tell a joke")
print(response.text)  # Just text, no JSON
```

---



## âš™ï¸ Environment Variables (optional)

| Variable           | Description                                 | Default Value |
|--------------------|---------------------------------------------|---------------|
| `GROK_COOKIES`     | Cookies file for GrokClient                 | `None`        |
| `GROK_PROXY`       | Proxy (e.g., `http://localhost:8080`)       | `None`        |
| `GROK_TIMEOUT`     | Grok request timeout (in seconds)           | `120`         |
| `GROK_SERVER_HOST` | IP address for running the server           | `0.0.0.0`     |
| `GROK_SERVER_PORT` | Port for running the server                 | `8000`        |

---

## ğŸ“‚ Project Structure

```
Grok3API/
â”œâ”€â”€ grok3api/
â”‚   â”œâ”€â”€ server.py          # <--- running the server
â”‚   â”œâ”€â”€ client.py
â”‚   â”œâ”€â”€ types/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ openai_test.py     # <--- compatibility test
â””â”€â”€ README.md
```

---

## â— TODO / Known Issues

- [ ] ğŸ”„ Streaming support (`stream=True`)
- [ ] ğŸ§ª More tests and validation
- [ ] ğŸ§¼ Refactor `message_payload` and history logic
- [ ] ğŸ§© Custom instructions, images, and additional features

---

ğŸ’¬ For questions and suggestions â€” feel free to open an issue!